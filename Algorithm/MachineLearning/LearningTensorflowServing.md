

# Requirements
1. 机器学习过程中从系统到应用的监控，其中包括:
* 整体计算资源特别是GPU的使用情况：使用率，内存，温度
* 每个机器学习应用的具体使用资源情况
* 机器学习过程的可视化
2. 快速高效的问题诊断
* 通过集中化日志的管理控制台进行轻松的一站式问题诊断
3. 一键式的失败恢复
* 从失败节点调度到可用节点
* 分布式存储保存计算中的checkpoint，可以随时在其它节点继续学习任务
4. 模型的持续改进和发布
* 利用分布式存储将模型无缝迁入生产环境
* 蓝绿发布
* 模型回滚


# TFJob
其实数据科学家运行模型训练关心的是三件事：
* 数据从哪里来
* 如何运行机器学习的代码
* 训练结果（模型和日志）到哪里去

对于运行在桌面机上的机器学习代码来说，这是很容易的事情；但是如果您的运行时环境是多台机器的集群环境，这个工作就变得复杂了。而TFJob希望机器学习训练的工程师们在集群的控制节点上实现中心化的配置管理，并且只需要了解如何编写和配置TFJob就能够像在单机上运行机器学习代码。

# Tensorflow分布式训练
TensorFlow作为现在最为流行的深度学习代码库，在数据科学家中间非常流行，特别是可以明显加速训练效率的分布式训练更是杀手级的特性。但是如何真正部署和运行大规模的分布式模型训练，却成了新的挑战。 实际分布式TensorFLow的使用者需要关心3件事情。
* 寻找足够运行训练的资源，通常一个分布式训练需要若干数量的worker(运算服务器)和ps(参数服务器)，而这些运算成员都需要使用计算资源。
* 安装和配置支撑程序运算的软件和应用
* 根据分布式TensorFlow的设计，需要配置ClusterSpec。这个json格式的ClusterSpec是用来描述整个分布式训练集群的架构，比如需要使用两个worker和ps，ClusterSpec应该长成下面的样子，并且分布式训练中每个成员都需要利用这个ClusterSpec初始化tf.train.ClusterSpec对象，建立集群内部通信

其中第一件事情是Kubernetes资源调度非常擅长的事情，无论CPU和GPU调度，都是直接可以使用；而第二件事情是Docker擅长的，固化和可重复的操作保存到容器镜像。而自动化的构建ClusterSpec是TFJob解决的问题，让用户通过简单的集中式配置，完成TensorFlow分布式集群拓扑的构建。

# Kubeflow

KubeFlow是基于Kubernetes构建的可组合，便携式, 可扩展的机器学习技术栈，支持实现从JupyterHub模型开发，TFJob模型训练到TF-serving，Seldon预测端到端的解决方案。

# Tensorflow Serving
TensorFlow Serving是Google开源的一个灵活的、高性能的机器学习模型服务系统，能够简化并加速从模型到生产应用的过程。它除了原生支持TensorFlow模型，还可以扩展支持其他类型的机器学习模型。

TensorFlow Serving的典型的流程如下：学习者(Learner，比如TensorFlow)根据输入数据进行模型训练。等模型训练完成、验证之后，模型会被发布到TensorFlow Serving系统服务器端。客户端提交请求，由服务端返回预测结果。客户端和服务端之间的通信采用的是RPC协议。

TensorFlow Serving是Google开源的一个灵活的、高性能的机器学习模型服务系统，能够简化并加速从模型到生产应用的过程。它除了原生支持TensorFlow模型，还可以扩展支持其他类型的机器学习模型。但是TensorFlow Serving支持的模型文件格式是protobuf，而不是TensorFlow模型训练产生的checkpoint文件。这就需要能够将模型训练产生的model.ckpt转化成.pb文件。
